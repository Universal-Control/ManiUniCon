_target_: maniunicon.policies.torch_model_vla.VLAPolicy

frame_latency: ${eval:'1 / 30'}
infer_latency: ${eval:'1 / 20'}
steps_per_inference: 3 # expected openloop steps
use_real_time_chunking: false
enable_recording: false
record_dir: null # path to recorded files
device: ${device}
synchronized: ${synchronized}

model:
  _target_: maniunicon.customize.policy_model.robovlms_model.RoboVlmsModel
  ckpt_path: null # path to robovlms model ckpt
  config_path: null # path to robovlms model config file
  device: ${device}
  log_save_dir: null # path to save ckpt loading msg, defult is null
  use_act_chunk: true
  task_name: "pick up anything" # task instruction that input to vla model

obs_wrapper:
  _target_: maniunicon.customize.obs_wrapper.robovlms_rgb_wrapper.RoboVlmsImageWrapper
  camera_config: ${data.camera_config}
  state_keys: ["tcp_position", "tcp_orientation", "joint_positions", "joint_velocities", "gripper_state"]
  num_cams: 2
  observation_horizon: 1 # should be the same as the window size in vla model

act_wrapper:
  _target_: maniunicon.customize.act_wrapper.robovlms_eepose_wrapper.RoboVlmsEEPoseWrapper
  hist_action: 0  # number of historical actions to consider
  action_horizon: 5 # should be the same as the action chunk size in vla model
  control_mode: cartesian
  dt: ${policy.dt}